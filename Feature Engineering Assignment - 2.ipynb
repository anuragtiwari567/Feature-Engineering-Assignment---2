{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6522c01-d223-476c-858f-66dbbc1c2145",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60390aa1-ba26-4a7e-9136-1c2c68106319",
   "metadata": {},
   "source": [
    "# Min-Max Scaling\n",
    "\n",
    "Min-Max scaling, also known as feature scaling, is a data preprocessing technique used to transform numerical features into a specific range. It ensures that all features have similar scales, which can be crucial for machine learning algorithms that rely on distance measures or weighted sums of input variables.\n",
    "\n",
    "Here’s how Min-Max scaling works:\n",
    "\n",
    "## Normalization:\n",
    "\n",
    "Min-Max scaling normalizes the data to a specific range, typically between 0 and 1.\n",
    "It transforms each feature value by subtracting the minimum value and dividing by the range (maximum - minimum).\n",
    "The formula for Min-Max scaling is: \n",
    "\\[ x_{\\text{scaled}} = \\frac{{x - x_{\\text{min}}}}{{x_{\\text{max}} - x_{\\text{min}}}} \\]\n",
    "\n",
    "## Application:\n",
    "\n",
    "Min-Max scaling is commonly used when features have different units or scales.\n",
    "It ensures that all features contribute equally to the model, regardless of their original ranges.\n",
    "\n",
    "## Example:\n",
    "\n",
    "Let’s consider a dataset with two features: “Age” and “Income.” We want to scale these features using Min-Max scaling. \n",
    "\n",
    "### Original data:\n",
    "\n",
    "| Age | Income |\n",
    "|-----|--------|\n",
    "| 30  | 50,000 |\n",
    "| 40  | 80,000 |\n",
    "| 25  | 60,000 |\n",
    "\n",
    "### Calculate the minimum and maximum values for each feature:\n",
    "\n",
    "\\(x_{\\text{min, Age}} = 25\\), \\(x_{\\text{max, Age}} = 40\\)\n",
    "\n",
    "\\(x_{\\text{min, Income}} = 50,000\\), \\(x_{\\text{max, Income}} = 80,000\\)\n",
    "\n",
    "### Apply Min-Max scaling:\n",
    "\n",
    "For “Age”: \n",
    "\\[ x_{\\text{scaled, Age}} = \\frac{{30 - 25}}{{40 - 25}} = 0.5 \\]\n",
    "\\[ x_{\\text{scaled, Age}} = \\frac{{40 - 25}}{{40 - 25}} = 1.0 \\]\n",
    "\\[ x_{\\text{scaled, Age}} = \\frac{{25 - 25}}{{40 - 25}} = 0.0 \\]\n",
    "\n",
    "For “Income”: \n",
    "\\[ x_{\\text{scaled, Income}} = \\frac{{50,000 - 50,000}}{{80,000 - 50,000}} = 0.0 \\]\n",
    "\\[ x_{\\text{scaled, Income}} = \\frac{{80,000 - 50,000}}{{80,000 - 50,000}} = 1.0 \\]\n",
    "\\[ x_{\\text{scaled, Income}} = \\frac{{60,000 - 50,000}}{{80,000 - 50,000}} = 0.2 \\]\n",
    "\n",
    "### Scaled data:\n",
    "\n",
    "| Age (Scaled) | Income (Scaled) |\n",
    "|--------------|-----------------|\n",
    "| 0.5          | 0.0             |\n",
    "| 1.0          | 1.0             |\n",
    "| 0.0          | 0.2             |\n",
    "\n",
    "Now both features are within the range [0, 1], making them suitable for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5e061-9f7a-47f6-9980-13a38dd5a05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ad25be4-c737-4b02-a9fb-008e09cc8dd4",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e4e3b-cfa3-4f77-8380-f40d5d54c494",
   "metadata": {},
   "source": [
    "# Feature Scaling Techniques: Unit Vector Scaling vs. Min-Max Scaling\n",
    "\n",
    "Both Unit Vector Scaling and Min-Max Scaling are feature scaling techniques used to preprocess data, but they have different approaches and use cases.\n",
    "\n",
    "## Unit Vector Scaling (Normalization):\n",
    "\n",
    "Unit Vector Scaling ensures that each feature vector has a unit length (i.e., a length of 1).\n",
    "It is particularly useful when dealing with features that have hard boundaries.\n",
    "For example, when working with image data, where color values range from 0 to 255, Unit Vector Scaling can be beneficial.\n",
    "Formula: \n",
    "\\[ x_{\\text{scaled}} = \\frac{{x}}{{|x|}} \\]\n",
    "Here, \\(|x|\\) represents the Euclidean norm (length) of the feature vector.\n",
    "Example: Suppose we have an RGB color vector ([100, 150, 200]). After Unit Vector Scaling, the vector becomes ([0.267, 0.401, 0.534]).\n",
    "\n",
    "## Min-Max Scaling:\n",
    "\n",
    "Min-Max Scaling (also known as normalization) scales features to a specific range, typically between 0 and 1.\n",
    "It maintains the relative relationship between values and preserves the shape of the distribution.\n",
    "Min-Max Scaling is commonly used in machine learning.\n",
    "Formula: \n",
    "\\[ x_{\\text{scaled}} = \\frac{{x - x_{\\text{min}}}}{{x_{\\text{max}} - x_{\\text{min}}}} \\]\n",
    "Here, \\(x_{\\text{min}}\\) and \\(x_{\\text{max}}\\) are the minimum and maximum values of the feature.\n",
    "Example: If we have an age feature ranging from 20 to 60, Min-Max Scaling maps it to [0, 1].\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "- Unit Vector Scaling:\n",
    "  - Scales the entire feature vector to have a unit length.\n",
    "  - Useful for features with hard boundaries.\n",
    "  - Ensures that all features contribute equally, regardless of their original ranges.\n",
    "\n",
    "- Min-Max Scaling:\n",
    "  - Scales features to a specific range (e.g., [0, 1]).\n",
    "  - Preserves the relative order of values.\n",
    "  - Commonly used for various machine learning algorithms.\n",
    "\n",
    "### Illustration:\n",
    "\n",
    "Let’s consider a dataset with two features: “Income” (ranging from 20,000 to 100,000) and “Age” (ranging from 0 to 100). We’ll apply both scaling techniques:\n",
    "\n",
    "#### Original data:\n",
    "\n",
    "| Income | Age |\n",
    "|--------|-----|\n",
    "| 50,000 | 30  |\n",
    "| 80,000 | 40  |\n",
    "| 60,000 | 25  |\n",
    "\n",
    "#### Unit Vector Scaling:\n",
    "\n",
    "Calculate the Euclidean norm for each feature vector.\n",
    "Divide each feature by its norm.\n",
    "\n",
    "##### Scaled data:\n",
    "\n",
    "| Income | Age   |\n",
    "|--------|-------|\n",
    "| 0.267  | 0.963 |\n",
    "| 0.428  | 0.903 |\n",
    "| 0.321  | 0.947 |\n",
    "\n",
    "#### Min-Max Scaling:\n",
    "\n",
    "Calculate the minimum and maximum values for each feature.\n",
    "Apply the Min-Max formula.\n",
    "\n",
    "##### Scaled data:\n",
    "\n",
    "| Income | Age |\n",
    "|--------|-----|\n",
    "| 0.0    | 0.3 |\n",
    "| 1.0    | 0.4 |\n",
    "| 0.2    | 0.0 |\n",
    "\n",
    "Both techniques ensure that features are within a specific range, but Unit Vector Scaling emphasizes unit length, while Min-Max Scaling focuses on relative scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db129df2-1933-42f3-b6c4-152e25705602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc0686cb-ddeb-4351-9367-c34e9ed639bb",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106dd6c-bf67-4fa5-8bf6-16166fec510c",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis)\n",
    "PCA is a statistical technique used for dimensionality reduction. It aims to transform a dataset into a new coordinate system, where the variables (or features) are represented by a set of linearly uncorrelated components called principal components. These components are ordered by the amount of variance they explain in the original data, with the first component capturing the most variance.\n",
    "\n",
    "PCA works by identifying the directions (or axes) along which the data varies the most. It then projects the original data onto these axes, reducing the dimensionality of the dataset while preserving the maximum amount of variability. This reduction in dimensionality can help in simplifying the dataset, removing noise, and focusing on the most important features.\n",
    "\n",
    "**Example:**\n",
    "Suppose you have a dataset containing information about houses, including features like size (in square feet), number of bedrooms, number of bathrooms, and distance from the city center. You want to reduce the dimensionality of this dataset while retaining as much information as possible.\n",
    "\n",
    "You can apply PCA to this dataset to find the principal components that capture the most variability. After applying PCA, you may find that the first principal component primarily represents the size of the house, the second principal component represents the number of bedrooms and bathrooms, and the third principal component represents the distance from the city center.\n",
    "\n",
    "By retaining only the first few principal components (say, the first two), you can effectively reduce the dimensionality of the dataset while still capturing a significant amount of information. This reduced representation can then be used for further analysis or modeling tasks, such as clustering or regression, with potentially improved performance and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca117b3-51e9-46c9-88f9-b22307da4c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed8f1ffb-406d-47b7-b9e8-d8a0bbc15f5a",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec434d6-04e7-44cd-9723-8d55f2da59f3",
   "metadata": {},
   "source": [
    "# PCA as a Feature Extraction Technique\n",
    "\n",
    "PCA (Principal Component Analysis) is a powerful technique used for dimensionality reduction, which is a form of feature extraction. Let’s explore the relationship between PCA and feature extraction, along with an illustrative example:\n",
    "\n",
    "## Feature Extraction:\n",
    "\n",
    "Feature extraction aims to transform the original set of features into a smaller set of more informative features.\n",
    "It helps reduce the dimensionality of the data while retaining essential information.\n",
    "Feature extraction techniques include creating new features, combining existing ones, or selecting a subset of features.\n",
    "\n",
    "## PCA as a Feature Extraction Technique:\n",
    "\n",
    "PCA is a specific method for feature extraction.\n",
    "It identifies a set of orthogonal axes (principal components) that capture the maximum variance in the data.\n",
    "These principal components are linear combinations of the original features.\n",
    "By projecting the data onto these components, we obtain a lower-dimensional representation.\n",
    "\n",
    "### How PCA Works for Feature Extraction:\n",
    "\n",
    "Given a dataset with (n) features, PCA computes the principal components:\n",
    "- The first principal component explains the most variance, the second explains the second most, and so on.\n",
    "- Principal components are uncorrelated (orthogonal) to each other.\n",
    "- We can choose to keep a subset of these components (e.g., the top (k) components) to reduce dimensionality.\n",
    "\n",
    "#### Illustrative Example:\n",
    "\n",
    "Let’s consider a dataset with three features: “Height,” “Weight,” and “Age.” We want to reduce it to two dimensions using PCA:\n",
    "\n",
    "### Original data (simplified):\n",
    "\n",
    "| Height | Weight | Age |\n",
    "|--------|--------|-----|\n",
    "| 170    | 65     | 30  |\n",
    "| 160    | 55     | 25  |\n",
    "| 175    | 70     | 35  |\n",
    "\n",
    "#### Step-by-step PCA:\n",
    "\n",
    "1. **Standardize the Data (optional but recommended):**\n",
    "   - Center the data by subtracting the mean from each feature.\n",
    "   - Divide by the standard deviation to scale the features.\n",
    "  \n",
    "2. **Compute Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized data.\n",
    "  \n",
    "3. **Eigendecomposition:**\n",
    "   - Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "   - Eigenvectors represent the principal components.\n",
    "   - Eigenvalues indicate the variance explained by each component.\n",
    "  \n",
    "4. **Select Principal Components:**\n",
    "   - Sort the eigenvectors by their corresponding eigenvalues (in descending order).\n",
    "   - Choose the top 2 eigenvectors (since we want 2 dimensions).\n",
    "  \n",
    "5. **Transform Data:**\n",
    "   - Multiply the original data by the selected eigenvectors to get the reduced representation.\n",
    "\n",
    "#### Resulting reduced data (2D):\n",
    "\n",
    "| PC1  | PC2  |\n",
    "|------|------|\n",
    "| -0.87| 0.12 |\n",
    "| -1.02| -0.10|\n",
    "| -0.15| 0.22 |\n",
    "\n",
    "Now we have a 2D representation of the data, capturing most of the variance.\n",
    "\n",
    "### Advantages of PCA for Feature Extraction:\n",
    "\n",
    "- **Noise Reduction**: PCA focuses on the most important patterns, filtering out noise.\n",
    "- **Visualization**: Reduced dimensions allow easy visualization.\n",
    "- **Data Preprocessing**: PCA simplifies high-dimensional data for further analysis.\n",
    "\n",
    "In summary, PCA is a valuable tool for feature extraction, especially when dealing with multicollinear or high-dimensional datasets. It helps us find a compact representation of the data while preserving essential information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b80512f-dc23-4639-b69a-d59c5b47038a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a91fee3e-6e51-4cdc-ab27-67fe809d2415",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be53c45-e71c-44fd-9557-370172668316",
   "metadata": {},
   "source": [
    "# Min-Max Scaling for Food Delivery Features\n",
    "\n",
    "Min-Max scaling (also known as normalization) is a valuable preprocessing step for building a recommendation system for a food delivery service. Let’s discuss how it works and how it can be applied:\n",
    "\n",
    "## What is Min-Max Scaling?\n",
    "\n",
    "Min-Max scaling is a technique that transforms numerical features to a specific range, typically between 0 and 1. It ensures that all features have similar scales, which is crucial for many machine learning algorithms.\n",
    "\n",
    "The transformation formula is as follows: \n",
    "\\[ x_{\\text{scaled}} = \\frac{{x - x_{\\text{min}}}}{{x_{\\text{max}} - x_{\\text{min}}}} \\]\n",
    "\n",
    "Here, \\(x\\) represents the original feature value, \\(x_{\\text{min}}\\) is the minimum value of that feature, and \\(x_{\\text{max}}\\) is the maximum value.\n",
    "\n",
    "## Applying Min-Max Scaling to Food Delivery Features:\n",
    "\n",
    "Let’s consider the features in your dataset:\n",
    "- **Price**: Represents the cost of the food item.\n",
    "- **Rating**: Indicates the user’s satisfaction with the restaurant.\n",
    "- **Delivery Time**: Reflects the estimated time for food delivery.\n",
    "\n",
    "### Steps for Min-Max Scaling:\n",
    "\n",
    "1. For each feature (Price, Rating, and Delivery Time):\n",
    "   - Calculate the minimum and maximum values in the dataset.\n",
    "2. Apply the Min-Max scaling formula to transform the feature values to the desired range (e.g., [0, 1]).\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose we have the following sample data (simplified for illustration):\n",
    "\n",
    "| Price | Rating | Delivery Time |\n",
    "|-------|--------|---------------|\n",
    "| 10    | 4.5    | 30            |\n",
    "| 20    | 3.8    | 45            |\n",
    "| 15    | 4.2    | 35            |\n",
    "\n",
    "#### Calculate the minimum and maximum values:\n",
    "\n",
    "- \\(x_{\\text{min, Price}} = 10\\), \\(x_{\\text{max, Price}} = 20\\)\n",
    "- \\(x_{\\text{min, Rating}} = 3.8\\), \\(x_{\\text{max, Rating}} = 4.5\\)\n",
    "- \\(x_{\\text{min, Delivery Time}} = 30\\), \\(x_{\\text{max, Delivery Time}} = 45\\)\n",
    "\n",
    "#### Apply Min-Max scaling:\n",
    "\n",
    "For “Price”:\n",
    "\\[ x_{\\text{scaled, Price}} = \\frac{{10 - 10}}{{20 - 10}} = 0.0 \\]\n",
    "\\[ x_{\\text{scaled, Price}} = \\frac{{20 - 10}}{{20 - 10}} = 1.0 \\]\n",
    "\\[ x_{\\text{scaled, Price}} = \\frac{{15 - 10}}{{20 - 10}} = 0.5 \\]\n",
    "\n",
    "For “Rating”:\n",
    "\\[ x_{\\text{scaled, Rating}} = \\frac{{4.5 - 3.8}}{{4.5 - 3.8}} = 1.0 \\]\n",
    "\\[ x_{\\text{scaled, Rating}} = \\frac{{3.8 - 3.8}}{{4.5 - 3.8}} = 0.0 \\]\n",
    "\\[ x_{\\text{scaled, Rating}} = \\frac{{4.2 - 3.8}}{{4.5 - 3.8}} = 0.5 \\]\n",
    "\n",
    "For “Delivery Time”:\n",
    "\\[ x_{\\text{scaled, Delivery Time}} = \\frac{{30 - 30}}{{45 - 30}} = 0.0 \\]\n",
    "\\[ x_{\\text{scaled, Delivery Time}} = \\frac{{45 - 30}}{{45 - 30}} = 1.0 \\]\n",
    "\\[ x_{\\text{scaled, Delivery Time}} = \\frac{{35 - 30}}{{45 - 30}} = 0.5 \\]\n",
    "\n",
    "Now the features are scaled within the desired range, making them suitable for recommendation system modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a9341-6afa-41f0-9dc6-d4008839a1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d044f222-6b1d-41f1-b22b-78e2830ad127",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918be2b-362b-4ec6-9956-8f517b59f46b",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) for Stock Price Prediction\n",
    "\n",
    "Principal Component Analysis (PCA) is a valuable technique for reducing the dimensionality of datasets, making it particularly useful for building stock price prediction models. Let’s explore how PCA works and how it can be applied:\n",
    "\n",
    "## What is PCA?\n",
    "\n",
    "PCA is a statistical method used for dimensionality reduction.\n",
    "It identifies the most important patterns (variance) in high-dimensional data and represents them using a smaller set of uncorrelated features (principal components).\n",
    "The goal is to capture as much variance as possible while reducing the number of features.\n",
    "\n",
    "## How Does PCA Work for Dimensionality Reduction?\n",
    "\n",
    "Given a dataset with (n) features, PCA computes the principal components:\n",
    "- The first principal component explains the most variance, the second explains the second most, and so on.\n",
    "- Principal components are orthogonal (uncorrelated) to each other.\n",
    "- We can choose to keep a subset of these components (e.g., the top (k) components) to reduce dimensionality.\n",
    "\n",
    "## Applying PCA to Stock Price Prediction:\n",
    "\n",
    "Let’s assume your dataset contains features related to company financials (e.g., revenue, profit, debt) and market trends (e.g., trading volume, volatility).\n",
    "Here’s how you can use PCA:\n",
    "\n",
    "### Step 1: Standardize the Data (optional but recommended):\n",
    "\n",
    "- Center the data by subtracting the mean from each feature.\n",
    "- Divide by the standard deviation to scale the features.\n",
    "\n",
    "### Step 2: Compute Covariance Matrix:\n",
    "\n",
    "Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "### Step 3: Eigendecomposition:\n",
    "\n",
    "- Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "- Eigenvectors represent the principal components.\n",
    "- Eigenvalues indicate the variance explained by each component.\n",
    "\n",
    "### Step 4: Select Principal Components:\n",
    "\n",
    "- Sort the eigenvectors by their corresponding eigenvalues (in descending order).\n",
    "- Choose the top (k) eigenvectors (where (k) is the desired reduced dimension).\n",
    "\n",
    "### Step 5: Transform Data:\n",
    "\n",
    "Multiply the original data by the selected eigenvectors to get the reduced representation.\n",
    "\n",
    "## Benefits of PCA for Stock Price Prediction:\n",
    "\n",
    "- **Dimensionality Reduction**: By selecting a subset of principal components, you reduce the number of features while retaining essential information.\n",
    "- **Noise Reduction**: PCA focuses on the most significant patterns, filtering out noise.\n",
    "- **Model Efficiency**: Smaller feature space speeds up model training and prediction.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose your dataset has 10 financial and market-related features. After applying PCA, you choose to keep the top 3 principal components. These components capture the most variance in the data, allowing you to build a more efficient stock price prediction model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583ed12-a8a4-4e96-b25b-5a0687f4b7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b1d88b5-ddea-4c32-8c99-b16e3023c795",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a36de3-6ddf-4585-bc26-b8cf82f3c050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[ 1  5 10 15 20]\n",
      "Scaled Data:\n",
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create the dataset\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = min_max_scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Original Data:\")\n",
    "print(data.flatten())\n",
    "print(\"Scaled Data:\")\n",
    "print(scaled_data.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688f3bf-9bce-45d8-b98a-1bf2dbf7403b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9d830ce-3763-4f8d-a241-b06ee501e03e",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a6af6-acde-4a04-ade8-de7a7c34e655",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) for Feature Extraction\n",
    "\n",
    "Principal Component Analysis (PCA) is a powerful technique for reducing the dimensionality of datasets while retaining important information. Let's explore how PCA can be applied to the given dataset with features: height, weight, age, gender, and blood pressure.\n",
    "\n",
    "## PCA for Feature Extraction:\n",
    "\n",
    "PCA aims to reduce the dimensionality of the dataset while retaining as much information as possible.\n",
    "It identifies a set of orthogonal axes (principal components) that capture the maximum variance in the data.\n",
    "The number of principal components to retain depends on the explained variance and the desired dimensionality reduction.\n",
    "\n",
    "### Steps for PCA:\n",
    "\n",
    "1. Standardize the data (optional but recommended).\n",
    "2. Compute the covariance matrix.\n",
    "3. Find the eigenvectors and eigenvalues.\n",
    "4. Sort the eigenvectors by eigenvalues (in descending order).\n",
    "5. Choose the top (k) eigenvectors (where (k) is the desired reduced dimension).\n",
    "\n",
    "### Choosing the Number of Principal Components:\n",
    "\n",
    "The optimal number of principal components depends on the explained variance.\n",
    "We often aim to retain a certain percentage of the total variance (e.g., 95% or 99%).\n",
    "You can calculate the cumulative explained variance and choose the smallest (k) that achieves the desired threshold.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose you compute the cumulative explained variance and find that the first 3 principal components explain 98% of the total variance.\n",
    "In this case, you might choose to retain these 3 components.\n",
    "\n",
    "#### Why Retain 3 Principal Components?\n",
    "\n",
    "- By retaining 3 components, you capture most of the variance while reducing the dimensionality.\n",
    "- It simplifies modeling, visualization, and interpretation.\n",
    "- Fewer dimensions lead to faster computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2e0a15-b82d-408c-bd58-e4870768b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio:\n",
      "[9.11579557e-01 8.84204433e-02 2.93793886e-31]\n",
      "Top 3 Principal Components:\n",
      "[[-3.48678002e+00  4.30221243e+00  5.71918286e-15]\n",
      " [ 1.37064816e+01 -1.21065941e+00  5.71918286e-15]\n",
      " [-1.02197016e+01 -3.09155302e+00  5.71918286e-15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample dataset (replace with your actual data)\n",
    "data = np.array([\n",
    "    [170, 65, 30, 1, 120],  # Height, Weight, Age, Gender, Blood Pressure\n",
    "    [160, 55, 25, 0, 130],\n",
    "    [175, 70, 35, 1, 125],\n",
    "    # Add more rows if needed\n",
    "])\n",
    "\n",
    "# Initialize PCA with desired number of components (e.g., 3)\n",
    "n_components = 3\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit and transform the data\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "# Print the explained variance ratio for each component\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Choose the top 3 principal components (or adjust as needed)\n",
    "print(\"Top 3 Principal Components:\")\n",
    "print(reduced_data)\n",
    "\n",
    "# Now you can use 'reduced_data' for modeling or visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6b310-ed6b-4f68-a02b-f710cfede9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5c486-536d-4255-8bd4-787a87721d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
